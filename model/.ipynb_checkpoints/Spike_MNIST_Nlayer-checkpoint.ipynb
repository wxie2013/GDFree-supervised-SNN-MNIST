{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# these command make the cell width wider than default\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spike_MNIST_Nlayer(input_layer, hidden_layer):\n",
    "    def __init__(self, task, idx_start_train, idx_start, idx_end, simulation_duration, epoch, previous_seg_name, sqrt_grp_size, test_option, hyperParams, debug, activate_input_spikemon, root_out):\n",
    "        if debug==True:\n",
    "            print('-------------------------------------------------')\n",
    "            print('..........', task, '..............')\n",
    "            print('-------------------------------------------------')\n",
    "            \n",
    "        self.rank, self.num_workers = get_rank_and_num_workers()  # rank of this worker and number of workers    \n",
    "        self.sqrt_grp_size = sqrt_grp_size\n",
    "        self.N_hidden = len(self.sqrt_grp_size)\n",
    "        self.model_name = 'Spike_MNIST_'+str(1+self.N_hidden)+'_layer'  # 1--> input\n",
    "        self.debug = debug\n",
    "        self.activate_input_spikemon = activate_input_spikemon\n",
    "        self.test_option = test_option\n",
    "        self.root_out = root_out\n",
    "        if self.test_option=='add_more_syn': # keep the same number of neurons but add more synapses\n",
    "                print('--- doing test with option: add_more_syn ---')\n",
    "                hyperParams['n_syn'] = self.num_workers        \n",
    "        \n",
    "        self.simulation_duration = simulation_duration\n",
    "        self.idx_start_train = idx_start_train\n",
    "        self.idx_start = idx_start\n",
    "        self.idx_end = idx_end\n",
    "        self.epoch = epoch\n",
    "        self.task = task \n",
    "        self.previous_seg_name = previous_seg_name # previous segment for consecutive training\n",
    "\n",
    "        self.model_success = self.manage_directories(hyperParams) # created all the directories\n",
    "\n",
    "        # if a model is not successfully created, don't run\n",
    "        if self.model_success == True: # call parent class after manage_directories is successful\n",
    "            super().__init__(hyperParams)\n",
    "            self.device_setting() # set device type.\n",
    "            self.set_equations(hyperParams)\n",
    "            self.build_neurons()\n",
    "            self.build_synapses()\n",
    "            self.set_monitors()\n",
    "            self.set_syn_active_status()                \n",
    "            self.collect()   \n",
    "            self.prep_run()\n",
    "    \n",
    "    # set the device before starting anything to build the model\n",
    "    def device_setting(self):\n",
    "        if input_param.device_name == 'cpp_standalone' or input_param.device_name == 'cuda_standalone':\n",
    "            directory = input_param.device_name+str(self.rank)   \n",
    "            if not isinstance(brian2.get_device(), CPPStandaloneDevice) and not isinstance(brian2.get_device(), CUDAStandaloneDevice):\n",
    "                brian2.set_device(input_param.device_name, directory=directory, build_on_run=False)\n",
    "            else:\n",
    "                brian2.device.reinit()\n",
    "                brian2.device.activate(directory=directory, build_on_run=False)\n",
    "            # too much memory consumed w/o this limit during cuda compilation \n",
    "            brian2.prefs['devices.cpp_standalone.extra_make_args_unix'] = ['-j8']\n",
    "        else:\n",
    "            sys.exit(\"!!!!! the option is not one of the following: 'cpp_standalone', 'cuda_standalone' !!!!!\")\n",
    "\n",
    "        # need this to finish the length of defined synases, \n",
    "        # https://brian.discourse.group/t/cant-modify-synaptic-group-attribute-using-run-args-in-standalone-mode/1219/13\n",
    "        brian2.seed(0)\n",
    "        \n",
    "    # compile the model before run\n",
    "    def prep_run(self):\n",
    "        # namespace for different stdp_type\n",
    "        if self.stdp_type == 0:\n",
    "            self.run_var_namespace = {'d_Apre':  self.d_Apre,\n",
    "                                 'd_Apost':  self.d_Apost,\n",
    "                                 'taupre':  self.taupre,\n",
    "                                 'taupost':  self.taupost, \n",
    "                                 'tau_adpt': self.tau_adpt}\n",
    "        elif self.stdp_type == 1:\n",
    "            self.run_var_namespace = {'nu_pre_ee': self.nu_pre_ee,      \n",
    "                                 'nu_post_ee': self.nu_post_ee,\n",
    "                                 'tc_pre_ee': self.tc_pre_ee, \n",
    "                                 'tc_post_1_ee': self.tc_post_1_ee,\n",
    "                                 'tc_post_2_ee': self.tc_post_2_ee}\n",
    "                        \n",
    "        # run now\n",
    "        if self.debug==True:\n",
    "            self.col.run(self.simulation_duration, \n",
    "                         report='text', \n",
    "                         namespace = self.run_var_namespace, \n",
    "                         profile=self.debug)\n",
    "        else:\n",
    "            brian2.BrianLogger.log_level_error() # don't show INFO and WARN level message\n",
    "            self.col.run(self.simulation_duration, \n",
    "                         report=None, \n",
    "                         namespace = self.run_var_namespace)\n",
    "            \n",
    "        brian2.device.build(run=False, directory=input_param.device_name+str(self.rank))   \n",
    "\n",
    "        \n",
    "    # restructure the data for model use\n",
    "    def manage_data(self, input_data):\n",
    "        num_samples = len(input_data['img'])\n",
    "        img_shape = input_data['img'][0].shape\n",
    "        img_array = np.empty((num_samples, img_shape[0] * img_shape[1]), dtype=input_data['img'][0].dtype)\n",
    "        label_array = np.empty(num_samples, dtype=input_data['label'][0].dtype)\n",
    "        for idx in range(num_samples):\n",
    "            img_array[idx] = input_data['img'][idx].reshape(-1)\n",
    "            label_array[idx] = input_data['label'][idx]\n",
    "\n",
    "        if self.debug:\n",
    "            print(' label: ', ', '.join([str(s) for s in label_array[:]]))\n",
    "\n",
    "        return img_array, label_array\n",
    "        \n",
    "    # define input and output directories for synapse, spike and adaptive threshold\n",
    "    def manage_directories(self, hyperParams):\n",
    "        # name contains scan parameter information\n",
    "        self.epoch_name = 'epoch_'+str(self.epoch)\n",
    "        self.seg_name = 'seg_'+str(self.idx_start_train+self.idx_start)+'_'+str(self.idx_start_train+self.idx_end) # current segment   \n",
    "        self.trial_dir = str(os.getcwd()) # different ray tune trial is in  different directory\n",
    "        # store analysis root output file\n",
    "\n",
    "        if self.root_out == True:\n",
    "            self.root_file_dir = os.path.join(self.trial_dir, \"root_file\", self.epoch_name, self.seg_name)\n",
    "            if os.path.isdir(self.root_file_dir) == False:\n",
    "                os.makedirs(self.root_file_dir)\n",
    "                \n",
    "            self.root_file = ROOT.TFile(os.path.join(self.root_file_dir, f\"eff_{self.task}_{self.seg_name}_rank{self.rank}.root\"), \"update\")\n",
    "            self.nt = self.root_file.Get(\"nt\")\n",
    "            if not self.nt:\n",
    "                self.nt = ROOT.TNtuple(\"nt\",  \"\", \"sidx:label:digit:rate:match:n_max:rank\")\n",
    "            \n",
    "        # directory to store neuron groups\n",
    "        self.dir_neuron_group = os.path.join(self.trial_dir,  'neuron_group')\n",
    "        if os.path.isdir(self.dir_neuron_group) == False: \n",
    "            os.makedirs(self.dir_neuron_group)\n",
    "\n",
    "        syn_out_name = 'syn_out'\n",
    "        \n",
    "        # directory to store trained synapses\n",
    "        self.dir_syn_out = os.path.join(self.trial_dir, syn_out_name, self.model_name, self.epoch_name, self.seg_name)\n",
    "        if os.path.isdir(self.dir_syn_out) == False and self.task=='train':\n",
    "            os.makedirs(self.dir_syn_out)\n",
    "        \n",
    "        # directory containing the final round of trained synapses\n",
    "        self.dir_syn_final = os.path.join(self.trial_dir, syn_out_name, self.model_name, 'final')\n",
    "                \n",
    "        self.dir_syn_in = str()\n",
    "        if self.task == 'train':                    \n",
    "            if  self.idx_start == 0 and self.epoch != 0: # load synapses from the last sample in the epoch before\n",
    "                self.dir_syn_in = os.path.join(self.trial_dir, syn_out_name, self.model_name, 'epoch_'+str(self.epoch-1), self.previous_seg_name)\n",
    "            elif self.idx_start > 0: # load synapse and threshold from trained synapses from previous sample segment in the same epoch\n",
    "                self.dir_syn_in = os.path.join(self.trial_dir, syn_out_name, self.model_name, self.epoch_name, self.previous_seg_name)\n",
    "            if len(self.dir_syn_in) >0:\n",
    "                if os.path.isdir(self.dir_syn_in) == False:\n",
    "                    os.makedirs(self.dir_syn_in)\n",
    "\n",
    "            # link the final directories\n",
    "            if self.rank == 0: \n",
    "                final_file = os.path.join(self.trial_dir, self.dir_syn_final)\n",
    "                file_exists = os.path.exists(final_file)\n",
    "                if file_exists==True:\n",
    "                    os.system(f'unlink {final_file}')\n",
    "            \n",
    "                lns_cmd = f'ln -s {self.dir_syn_out} {final_file}'\n",
    "                os.system(lns_cmd)\n",
    "                if self.debug==True:\n",
    "                    print(' link the final trained synapses: ', lns_cmd)        \n",
    "\n",
    "        else: # now testing or validing\n",
    "            self.dir_syn_in = os.path.join(self.trial_dir, self.dir_syn_final) \n",
    "\n",
    "        if hyperParams is None: \n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def build_neurons(self):\n",
    "        super().build_neurons()\n",
    "                   \n",
    "    def build_synapses(self):\n",
    "        super().build_synapses()\n",
    "        self.build_synapses_between_input_hidden()\n",
    "        self.build_synapses_between_hidden()\n",
    "        \n",
    "        \n",
    "    # synapse between input-hidden\n",
    "    def build_synapses_between_input_hidden(self):\n",
    "        # input neuron to excitatory\n",
    "        self.S_input2e = []\n",
    "        for ih in range(self.N_hidden):\n",
    "            self.S_input2e.append(brian2.Synapses(self.input_neurons, self.exci_neurons[ih], self.eqs_syn, on_pre=self.STDP_pre_action,\n",
    "                                                  on_post=self.STDP_post_action, name='s_input2e'+str(ih)))\n",
    "        \n",
    "        if self.idx_start==0 and self.epoch==0 and self.task=='train':\n",
    "            if self.debug==True:\n",
    "                print(' building (from scratch) synapses between input and hidden layers ...')\n",
    "            \n",
    "            for ih in range(len(self.S_input2e)):\n",
    "                self.S_input2e[ih].connect(p = 1, n=self.n_syn, skip_if_invalid=True)\n",
    "                self.S_input2e[ih].gmax = self.gmax_input2e[ih]\n",
    "                self.S_input2e[ih].max_delay = self.max_delay_input2e[ih]\n",
    "                self.S_input2e[ih].max_dendritic_delay = self.max_dendritic_delay\n",
    "                self.S_input2e[ih].w = 'rand() * gmax'\n",
    "                self.S_input2e[ih].pre_plastic.delay = 'rand()* max_delay'\n",
    "                self.S_input2e[ih].post.delay = 'rand()* max_dendritic_delay' # dendritic delay\n",
    "                self.S_input2e[ih].penalty = self.penalty_input2e[ih]\n",
    "                self.S_input2e[ih].w_sat_scale = self.w_sat_scale\n",
    "                self.S_input2e[ih].w_sat_shift = self.w_sat_shift\n",
    "        else:\n",
    "            self.load_synapses_between_input_hidden()\n",
    "\n",
    "    # synapse between input-auxiliary\n",
    "    def build_synapses_between_input_aux(self):\n",
    "        # input neuron to excitatory\n",
    "        self.S_input2aux = []\n",
    "        for ih in range(self.N_hidden):\n",
    "            self.S_input2aux.append(brian2.Synapses(self.input_neurons, self.aux_neurons[ih], self.eqs_syn, on_pre=self.STDP_pre_action,\n",
    "                                                  on_post=self.STDP_post_action, name='s_input2aux'+str(ih)))\n",
    "        \n",
    "        if self.idx_start==0 and self.epoch==0 and self.task=='train':\n",
    "            if self.debug==True:\n",
    "                print(' building (from scratch) synapses between input and hidden layers ...')\n",
    "                \n",
    "            prob = 'exp(-((x_pre-x_post)**2 + (y_pre-y_post)**2)/(2*(sigma)**2))\n",
    "            for ih in range(len(self.S_input2aux)):\n",
    "                self.S_input2aux[ih].connect(p = prob, n=self.n_syn, skip_if_invalid=True, namespace={'sigma':self.Receptive_field[ih]})\n",
    "                self.S_input2aux[ih].gmax = self.gmax_input2aux[ih]\n",
    "                self.S_input2aux[ih].max_delay = self.max_delay_input2aux[ih]\n",
    "                self.S_input2aux[ih].max_dendritic_delay = self.max_dendritic_delay\n",
    "                self.S_input2aux[ih].w = 'rand() * gmax'\n",
    "                self.S_input2aux[ih].pre_plastic.delay = 'rand()* max_delay'\n",
    "                self.S_input2aux[ih].post.delay = 'rand()* max_dendritic_delay' # dendritic delay\n",
    "                self.S_input2aux[ih].penalty = self.penalty_input2aux[ih]\n",
    "                self.S_input2aux[ih].w_sat_scale = 0\n",
    "                self.S_input2aux[ih].w_sat_shift = 0\n",
    "        else:\n",
    "            self.load_synapses_between_input_aux()\n",
    "\n",
    "\n",
    "    # compile loaded syn data for testing \n",
    "    def compile_read_syn_data(self, syn_path, ih_pre = None, ih_post = None):\n",
    "        if os.path.exists(syn_path) is False:\n",
    "            return None\n",
    "        #    \n",
    "        if self.test_option==None:\n",
    "            trained_syn = read_data(syn_path, self.rank)\n",
    "        elif self.test_option == 'use_rank0_info':\n",
    "            trained_syn = read_data(syn_path, 0)\n",
    "        elif self.test_option==\"add_more_syn\": # add more syns between a pair of pre- and post-neurons\n",
    "            if self.task != 'test' and self.task != 'advatk' and self.task != 'adv_make':\n",
    "                sys.exit(f'!!!! self.test_option != None in task: {self.task}, but it is required to be in task: test or advatk or adv_make !!!')\n",
    "            \n",
    "            list_dict = [None]*self.n_syn\n",
    "            for i in range(self.n_syn):\n",
    "                list_dict[i] = read_data(syn_path, i)\n",
    "                \n",
    "            if not None in list_dict:\n",
    "                trained_syn = concatenate_dict_arrays(list_dict)\n",
    "            else:\n",
    "                trained_syn = None\n",
    "                if self.debug == True: \n",
    "                    print(f'!!!! trained_syn at {syn_path} is None !!!!')\n",
    "        elif self.test_option != None and \"add_more_neuron\" in self.test_option:\n",
    "            if self.task != 'test' and self.task != 'advatk' and self.task != 'adv_make':\n",
    "                sys.exit(f'!!!! self.test_option != None in task: {self.task}, but it is required to be in task: test or advatk  or adv_make !!!')\n",
    "            \n",
    "            list_dict = []\n",
    "            for ir in range(self.num_workers):\n",
    "                syn = read_data(syn_path, ir)\n",
    "                if not syn:\n",
    "                    list_dict.append(syn)\n",
    "                    break\n",
    "                # change the index of pre-post neurons by i+rank*self.N_e[ih]\n",
    "                if 'S_input2e' in syn_path: # The number of input neurons, which is the pre-neuron, is fixed. \n",
    "                    syn['j'] = syn['j'] + ir*self.N_e[ih_post]\n",
    "                else: # both pre and post neurons are increased\n",
    "                    syn['i'] = syn['i'] + ir*self.N_e[ih_pre]\n",
    "                    syn['j'] = syn['j'] + ir*self.N_e[ih_post]\n",
    "                \n",
    "                list_dict.append(syn)\n",
    "                \n",
    "            if not None in list_dict:\n",
    "                trained_syn = concatenate_dict_arrays(list_dict)\n",
    "            else:\n",
    "                trained_syn = None\n",
    "                if self.debug == True: \n",
    "                    print(f'!!!! trained_syn at {syn_path} is None !!!!')\n",
    "        else:\n",
    "            sys.exit(f' !!!!  test option: {self.test_option} does not exist !!!!') \n",
    "        \n",
    "        if self.debug==True and self.task!='train':\n",
    "            write_data(pd.DataFrame.from_dict(trained_syn), syn_path, -1)\n",
    "            \n",
    "        return trained_syn\n",
    "            \n",
    "    # load synapses\n",
    "    def load_synapses_between_input_hidden(self):\n",
    "        for ih in range(len(self.S_input2e)):\n",
    "            trained_S_input2e_path = os.path.join(self.dir_syn_in, 'S_input2e'+str(ih))\n",
    "            trained_S_input2e = self.compile_read_syn_data(trained_S_input2e_path, ih_post = ih)\n",
    "            \n",
    "            if self.debug==True:\n",
    "                print(f'--loading trained synapses between input and hiddenlayer {ih}: ', trained_S_input2e_path)\n",
    "                \n",
    "            if trained_S_input2e is not None:\n",
    "                self.S_input2e[ih].connect(i=trained_S_input2e['i'], j=trained_S_input2e['j'])\n",
    "                self.S_input2e[ih].w[:] = trained_S_input2e['w']\n",
    "                self.S_input2e[ih].post.delay[:] = trained_S_input2e['delay_dendritic']*brian2.second\n",
    "                if self.task == 'train':\n",
    "                    self.S_input2e[ih].gmax = self.gmax_input2e[ih] # need to set these gmax value to avoid being cliped in pre-action\n",
    "                    self.S_input2e[ih].pre_plastic.delay[:] = trained_S_input2e['delay']*brian2.second\n",
    "                    self.S_input2e[ih].penalty = self.penalty_input2e[ih]\n",
    "                    self.S_input2e[ih].w_sat_scale = self.w_sat_scale\n",
    "                    self.S_input2e[ih].w_sat_shift = self.w_sat_shift\n",
    "                else:\n",
    "                    self.S_input2e[ih].pre_nonplastic.delay[:] = trained_S_input2e['delay']*brian2.second\n",
    "            else:\n",
    "                self.S_input2e[ih].connect(False)\n",
    "            \n",
    "    # load synapses\n",
    "    def load_synapses_between_input_aux(self):\n",
    "        for ih in range(len(self.S_input2aux)):\n",
    "            trained_S_input2aux_path = os.path.join(self.dir_syn_in, 'S_input2aux'+str(ih))\n",
    "            trained_S_input2aux = self.compile_read_syn_data(trained_S_input2aux_path, ih_post = ih)\n",
    "            \n",
    "            if self.debug==True:\n",
    "                print(f'--loading trained synapses between input and hiddenlayer {ih}: ', trained_S_input2aux_path)\n",
    "                \n",
    "            if trained_S_input2aux is not None:\n",
    "                self.S_input2aux[ih].connect(i=trained_S_input2aux['i'], j=trained_S_input2aux['j'])\n",
    "                self.S_input2aux[ih].w[:] = trained_S_input2aux['w']\n",
    "                self.S_input2aux[ih].post.delay[:] = trained_S_input2aux['delay_dendritic']*brian2.second\n",
    "                if self.task == 'train':\n",
    "                    self.S_input2aux[ih].gmax = self.gmax_input2e[ih] # need to set these gmax value to avoid being cliped in pre-action\n",
    "                    self.S_input2aux[ih].pre_plastic.delay[:] = trained_S_input2aux['delay']*brian2.second\n",
    "                    self.S_input2aux[ih].penalty = self.penalty_input2e[ih]\n",
    "                    self.S_input2aux[ih].w_sat_scale = self.w_sat_scale\n",
    "                    self.S_input2aux[ih].w_sat_shift = self.w_sat_shift\n",
    "                else:\n",
    "                    self.S_input2aux[ih].pre_nonplastic.delay[:] = trained_S_input2aux['delay']*brian2.second\n",
    "            else:\n",
    "                self.S_input2aux[ih].connect(False)\n",
    "    \n",
    "    def build_synapses_between_hidden(self):\n",
    "        self.S_efe = [] # current layer excitatory to next layer excitatory neurons\n",
    "        for ih in range(1, self.N_hidden):\n",
    "            deep_connection = []  #the ih layer is connected to all earlier layer\n",
    "            for jh in range(ih):\n",
    "                deep_connection.append(brian2.Synapses(self.exci_neurons[jh], self.exci_neurons[ih], self.eqs_syn, on_pre=self.STDP_pre_action, \n",
    "                                                  on_post=self.STDP_post_action, name='s_efe'+str(jh)+\"to\"+str(ih)))\n",
    "            self.S_efe.append(deep_connection)\n",
    "                    \n",
    "        if self.idx_start==0 and self.epoch==0 and self.task=='train':\n",
    "            if self.debug==True:\n",
    "                print(' building (from scratch) synapses between hidden layer ...')\n",
    "            for i in range(len(self.S_efe)):\n",
    "                for j in range(len(self.S_efe[i])):\n",
    "                    self.S_efe[i][j].connect(condition='group_pre==group_post and rank_pre==rank_post')\n",
    "                    self.S_efe[i][j].gmax = self.gmax_efe[i]\n",
    "                    self.S_efe[i][j].max_delay = self.max_delay_efe[i]\n",
    "                    self.S_efe[i][j].max_dendritic_delay = self.max_dendritic_delay\n",
    "                    self.S_efe[i][j].w = 'rand() * gmax'\n",
    "                    self.S_efe[i][j].pre_plastic.delay = 'rand()* max_delay'\n",
    "                    self.S_efe[i][j].post.delay = 'rand()* max_dendritic_delay'\n",
    "                    self.S_efe[i][j].penalty = self.penalty_efe[i]\n",
    "                    self.S_efe[i][j].w_sat_scale = self.w_sat_scale\n",
    "                    self.S_efe[i][j].w_sat_shift = self.w_sat_shift\n",
    "        else: # synpases has been created from the previous round\n",
    "            self.load_synapses_between_hidden()\n",
    "\n",
    "    def build_synapses_between_aux(self):\n",
    "        self.S_auxfaux = [] # current layer excitatory to next layer excitatory neurons\n",
    "        for ih in range(1, self.N_hidden):\n",
    "            deep_connection = []  #the ih layer is connected to all earlier layer\n",
    "            for jh in range(ih):\n",
    "                deep_connection.append(brian2.Synapses(self.aux_neurons[jh], self.aux_neurons[ih], self.eqs_syn, on_pre=self.STDP_pre_action, \n",
    "                                                  on_post=self.STDP_post_action, name='s_auxfaux'+str(jh)+\"to\"+str(ih)))\n",
    "            self.S_auxfaux.append(deep_connection)\n",
    "                    \n",
    "        if self.idx_start==0 and self.epoch==0 and self.task=='train':\n",
    "            if self.debug==True:\n",
    "                print(' building (from scratch) synapses between auxiliary layer ...')\n",
    "            for i in range(len(self.S_auxfaux)):\n",
    "                for j in range(len(self.S_efe[i])):\n",
    "                    self.S_auxfaux[i][j].connect(condition='group_pre==group_post and rank_pre==rank_post')\n",
    "                    self.S_auxfaux[i][j].gmax = self.gmax_auxfaux[i]\n",
    "                    self.S_auxfaux[i][j].max_delay = self.max_delay_auxfaux[i]\n",
    "                    self.S_auxfaux[i][j].max_dendritic_delay = self.max_dendritic_delay\n",
    "                    self.S_auxfaux[i][j].w = 'rand() * gmax'\n",
    "                    self.S_auxfaux[i][j].pre_plastic.delay = 'rand()* max_delay'\n",
    "                    self.S_auxfaux[i][j].post.delay = 'rand()* max_dendritic_delay'\n",
    "                    self.S_auxfaux[i][j].penalty = self.penalty_auxfaux[i]\n",
    "                    self.S_auxfaux[i][j].w_sat_scale = 0\n",
    "                    self.S_auxfaux[i][j].w_sat_shift = 0\n",
    "        else: # synpases has been created from the previous round\n",
    "            self.load_synapses_between_aux()\n",
    "\n",
    "    # ## load connections among hidden layers\n",
    "    def load_synapses_between_hidden(self):\n",
    "        for i in range(len(self.S_auxfaux)):\n",
    "            for j in range(len(self.S_auxfaux[i])):\n",
    "                trained_S_auxfaux_path = os.path.join(self.dir_syn_in, 'S_auxfaux'+str(j)+\"to\"+str(i+1))\n",
    "                trained_S_auxfaux = self.compile_read_syn_data(trained_S_auxfaux_path, ih_pre=j, ih_post=i+1)\n",
    "\n",
    "                if self.debug==True:\n",
    "                    print('-- loading trained synapses between auxiliary layers {i} and {j}: ', trained_S_auxfaux)            \n",
    "                \n",
    "                if trained_S_auxfaux is not None:\n",
    "                    self.S_auxfaux[i][j].connect(i=trained_S_auxfaux['i'], j=trained_S_auxfaux['j'])\n",
    "                    self.S_auxfaux[i][j].w[:] = trained_S_auxfaux['w']\n",
    "                    self.S_auxfaux[i][j].post.delay[:] = trained_S_auxfaux['delay_dendritic']*brian2.second\n",
    "                    if self.task == 'train':\n",
    "                        self.S_auxfaux[i][j].gmax = self.gmax_auxfaux[i]  # need this to avoid weight being cliped in pre-action\n",
    "                        self.S_auxfaux[i][j].pre_plastic.delay[:] = trained_S_auxfaux['delay']*brian2.second\n",
    "                        self.S_auxfaux[i][j].penalty = self.penalty_auxfaux[i]\n",
    "                        self.S_auxfaux[i][j].w_sat_scale = 0\n",
    "                        self.S_auxfaux[i][j].w_sat_shift =0\n",
    "                    else:\n",
    "                        self.S_auxfaux[i][j].pre_nonplastic.delay[:] = trained_S_auxfaux['delay']*brian2.second\n",
    "                else:\n",
    "                    self.S_auxfaux[i][j].connect(False)\n",
    "                    \n",
    "    # ## load connections among hidden layers\n",
    "    def load_synapses_between_aux(self):\n",
    "        for i in range(len(self.S_efe)):\n",
    "            for j in range(len(self.S_efe[i])):\n",
    "                trained_S_efe_path = os.path.join(self.dir_syn_in, 'S_efe'+str(j)+\"to\"+str(i+1))\n",
    "                trained_S_efe = self.compile_read_syn_data(trained_S_efe_path, ih_pre=j, ih_post=i+1)\n",
    "\n",
    "                if self.debug==True:\n",
    "                    print('-- loading trained synapses between hidden layers {i} and {j}: ', trained_S_efe)            \n",
    "                \n",
    "                if trained_S_efe is not None:\n",
    "                    self.S_efe[i][j].connect(i=trained_S_efe['i'], j=trained_S_efe['j'])\n",
    "                    self.S_efe[i][j].w[:] = trained_S_efe['w']\n",
    "                    self.S_efe[i][j].post.delay[:] = trained_S_efe['delay_dendritic']*brian2.second\n",
    "                    if self.task == 'train':\n",
    "                        self.S_efe[i][j].gmax = self.gmax_efe[i]  # need this to avoid weight being cliped in pre-action\n",
    "                        self.S_efe[i][j].pre_plastic.delay[:] = trained_S_efe['delay']*brian2.second\n",
    "                        self.S_efe[i][j].penalty = self.penalty_efe[i]\n",
    "                        self.S_efe[i][j].w_sat_scale = self.w_sat_scale\n",
    "                        self.S_efe[i][j].w_sat_shift = self.w_sat_shift\n",
    "                    else:\n",
    "                        self.S_efe[i][j].pre_nonplastic.delay[:] = trained_S_efe['delay']*brian2.second\n",
    "                else:\n",
    "                    self.S_efe[i][j].connect(False)\n",
    "                    \n",
    "                    \n",
    "    # monitors\n",
    "    def set_monitors(self):           \n",
    "        super().set_monitors()\n",
    "        record_list = [i for i in range(20, 30)]\n",
    "        if self.debug==True:\n",
    "            if self.task  == 'train':\n",
    "                self.S_input2e_mon = brian2.StateMonitor(self.S_input2e[0], ['penalty'], record = [0])\n",
    "    \n",
    "    # need to collect all objects so that Brian2 know what to run. The magic option doesn't work with self.\n",
    "    def collect(self):        \n",
    "        super().collect()\n",
    "        self.col.add(self.S_input2e)\n",
    "        self.col.add(self.S_efe)\n",
    "        if self.switch_aux == True:\n",
    "            self.col.add(self.S_inputfaux)\n",
    "            self.col.add(self.S_auxfaux)\n",
    "        if self.task == 'train' and self.debug == True: # only appears in training\n",
    "            self.col.add(self.S_input2e_mon)\n",
    "                    \n",
    "    # save the neurons. \n",
    "    def save_neurons(self):\n",
    "        super().save_neurons()\n",
    "                \n",
    "    # save the trained synapses. \n",
    "    def save_synapses(self):\n",
    "        super().save_synapses()\n",
    "        \n",
    "        if self.debug==True:\n",
    "            print('saving to: ', self.dir_syn_out)\n",
    "            \n",
    "        for ih in range(len(self.S_input2e)):\n",
    "            df = pd.DataFrame({'i': self.S_input2e[ih].i[:], \n",
    "                               'j': self.S_input2e[ih].j[:], \n",
    "                               'w': self.S_input2e[ih].w[:],\n",
    "                               'delay': self.S_input2e[ih].pre_plastic.delay[:],\n",
    "                               'delay_dendritic': self.S_input2e[ih].post.delay[:]})\n",
    "            write_data(df, os.path.join(self.dir_syn_out, 'S_input2e'+str(ih)), self.rank)\n",
    "        #\n",
    "        for i in range(len(self.S_efe)):\n",
    "            for j in range(len(self.S_efe[i])):\n",
    "                df = pd.DataFrame({'i': self.S_efe[i][j].i[:], \n",
    "                                   'j': self.S_efe[i][j].j[:], \n",
    "                                   'w': self.S_efe[i][j].w[:],\n",
    "                                   'delay': self.S_efe[i][j].pre_plastic.delay[:],\n",
    "                                   'delay_dendritic': self.S_efe[i][j].post.delay[:]})\n",
    "                write_data(df, os.path.join(self.dir_syn_out, 'S_efe'+str(j)+\"to\"+str(i+1)), self.rank)\n",
    "            \n",
    "    # plot the result, slow and mostly for debugging purpose\n",
    "    def plot(self):\n",
    "        super().plot()\n",
    "        # print synapse connection for S_input2e\n",
    "        for i in range(self.N_e[0]):\n",
    "            exci_idx = np.random.randint(0, self.N_e[0])\n",
    "            #print('igrp: ', igrp, 'x, y: ', self.exci_neurons[0].x[exci_idx], self.exci_neurons[0].y[exci_idx])\n",
    "            #print('input: (x,y): ', *zip(self.input_neurons.x[self.S_input2e[0].i[:, exci_idx]], self.input_neurons.y[self.S_input2e[0].i[:, exci_idx]]))\n",
    "        if self.task == 'train':\n",
    "                brian2.plt.figure(figsize=(10, 10))\n",
    "                brian2.plot(self.S_input2e_mon.t/brian2.ms, self.S_input2e_mon.penalty.T)\n",
    "                brian2.plt.xlabel('Time (ms)')\n",
    "                brian2.plt.ylabel('S_input2e penalty')\n",
    "                \n",
    "    # set active = True/False for pre and post in syn for training and testing\n",
    "    def set_syn_active_status(self):\n",
    "        super().set_syn_active_status()\n",
    "        \n",
    "        if self.task == 'train':\n",
    "            for ih in range(len(self.S_input2e)):\n",
    "                self.S_input2e[ih].pre_nonplastic.active = False  # fixed weight for testing only\n",
    "                self.S_input2e[ih].pre_plastic.active = True  # weight changing, for training\n",
    "                self.S_input2e[ih].post.active = True  # weight changing, for training\n",
    "            \n",
    "            for i in range(len(self.S_efe)):\n",
    "                for j in range(len(self.S_efe[i])):\n",
    "                    self.S_efe[i][j].pre_nonplastic.active = False  # fixed weight for testing only\n",
    "                    self.S_efe[i][j].pre_plastic.active = True  # weight changing, for training\n",
    "                    self.S_efe[i][j].post.active = True  # weight changing, for training\n",
    "        else:\n",
    "            for ih in range(len(self.S_input2e)):\n",
    "                self.S_input2e[ih].pre_nonplastic.active = True # fixed weight for testing only\n",
    "                self.S_input2e[ih].pre_plastic.active = False # weight changing, for training\n",
    "                self.S_input2e[ih].post.active = False  # weight changing, for training\n",
    "            \n",
    "            for i in range(len(self.S_efe)):\n",
    "                for j in range(len(self.S_efe[i])):\n",
    "                    self.S_efe[i][j].pre_nonplastic.active = True  # fixed weight for testing only\n",
    "                    self.S_efe[i][j].pre_plastic.active = False  # weight changing, for training\n",
    "                    self.S_efe[i][j].post.active = False  # weight changing, for training\n",
    "\n",
    "    # normalize weight after every sample\n",
    "    def syn_weight_normalization(self):\n",
    "        if self.debug==True:\n",
    "            print('...rank: ', self.rank, ' normalizing weight of S_input2e')\n",
    "        \n",
    "        S_input2e_w = list()\n",
    "        for ih in range(len(self.S_input2e)):\n",
    "            S_input2e_w.append(get_matrix_from_synapse_and_normalize(self.S_input2e[ih], self.gmax_input2e[ih], self.norm_scale_S_input2e[ih]))\n",
    "\n",
    "        if self.debug==True:\n",
    "            print('...rank: ', self.rank, ' normalizing weight of syn between hidden layers')\n",
    "\n",
    "        S_efe_w = []\n",
    "        for i in range(len(self.S_efe)):\n",
    "            S_efe_w.append([get_matrix_from_synapse_and_normalize(self.S_efe[i][j], self.gmax_efe[i], self.norm_scale_S_efe[i]) for j in range(len(self.S_efe[i]))])\n",
    "\n",
    "        return S_input2e_w, S_efe_w\n",
    "        \n",
    "    # check if the last run for the sample meet the requirements\n",
    "    def check_sample_status(self, img, label, stimulus_strength):\n",
    "        # if any hidden layer has spikes < n_spike_sample_min, increase the image strength until max_repetitions is reached.\n",
    "        counts_layers = [getattr(self.spike_counter, f'spike_counter_layer{ih}') for ih in range(self.N_hidden)]\n",
    "        higher_than_min_spike_rate = all(count >= input_param.n_spike_sample_min for count in counts_layers) \n",
    "            \n",
    "        if self.task == 'train':\n",
    "            if label == -1: # this is the first events which is fake for the purpose of reading attribute before run\n",
    "                return True, stimulus_strength\n",
    "                \n",
    "            counts_grps = [getattr(self.spike_counter, f'spike_counter_grp{ig}') for ig in range(input_param.num_classes)] \n",
    "            rate_correct_group = counts_grps[label] # rate of the group with id equal to label\n",
    "            correct_identification = (rate_correct_group == max(counts_grps)) and (counts_grps.count(rate_correct_group) ==1)\n",
    "            switch_stimulus = higher_than_min_spike_rate and correct_identification or (self.repetitions >=  input_param.max_repetitions)        \n",
    "        else:\n",
    "            switch_stimulus = higher_than_min_spike_rate or (self.repetitions >= input_param.max_repetitions)\n",
    "        \n",
    "        self.repetitions = int(not switch_stimulus)*(self.repetitions + 1)\n",
    "        stimulus_strength = input_param.scale_img*int(switch_stimulus) + (stimulus_strength+input_param.d_scale_img)*int(not switch_stimulus)\n",
    "\n",
    "        if self.debug==True:\n",
    "            message = f'''\n",
    "            higher_than_min_spike_rate: {higher_than_min_spike_rate} \n",
    "            repetitions: {self.repetitions}\n",
    "            '''\n",
    "            if self.task == 'train': \n",
    "                message += f'''\n",
    "                rate_correct_group: {rate_correct_group}\n",
    "                rate of_all_grps: {counts_grps}\n",
    "                correct_identification: {correct_identification}\n",
    "                '''\n",
    "            print(message)\n",
    "            \n",
    "        return switch_stimulus, stimulus_strength\n",
    "        \n",
    "    # run the model \n",
    "    def run(self, input_data):\n",
    "        # if a model is not successfully created, don't run\n",
    "        if self.model_success == False:\n",
    "            if self.debug==True:\n",
    "                print('!!! model_success == False !!! ')\n",
    "            return\n",
    "        #\n",
    "        img_array, label_array = self.manage_data(input_data)\n",
    "        data_in = [[img, label] for img, label in zip(img_array, label_array)]\n",
    "        list_counts_for_digit_last_layer = [None]*len(data_in) # n_spikes for each digit in the last h-layer from all samples\n",
    "        data_in.insert(0, [0, -1]) # fake same to run so that one can normalize syn before running any new samples\n",
    "        vt = [None]*self.N_hidden\n",
    "        for stimulus_idx, (img, label) in enumerate(tqdm_ray.tqdm(data_in)):\n",
    "            switch_stimulus = False\n",
    "            stimulus_strength = input_param.scale_img # starting strength\n",
    "            self.repetitions = 0 # num of repetitions for one sample\n",
    "            while switch_stimulus == False:\n",
    "                scaled_img = img*stimulus_strength\n",
    "                arguments = {\n",
    "                    self.input_neurons.rate: scaled_img*brian2.Hz, \n",
    "                    self.input_neurons.label: label, \n",
    "                    self.input_neurons.stimulus_idx: stimulus_idx, \n",
    "                    self.input_neurons.stimulus_strength:stimulus_strength, \n",
    "                }\n",
    "                for ih in range(self.N_hidden):\n",
    "                    arguments.update({\n",
    "                        self.exci_neurons[ih].stimulus_idx: stimulus_idx,\n",
    "                        self.exci_neurons[ih].stimulus_strength: stimulus_strength,\n",
    "                        self.exci_neurons[ih].label: label,\n",
    "                    })\n",
    "                    \n",
    "                if label!=-1: # skip the fake stimulus\n",
    "                    arguments.update({self.exci_neurons[ih].vt: vt[ih][:]})\n",
    "                    \n",
    "                    if self.task == 'train': \n",
    "                        for ih in range(len(self.S_input2e)):\n",
    "                            arguments.update({\n",
    "                                self.S_input2e[ih].w: S_input2e_w[ih][:], \n",
    "                            })\n",
    "                        for i in range(len(self.S_efe)):\n",
    "                            for j in range(len(self.S_efe[i])):\n",
    "                                arguments.update({\n",
    "                                    self.S_efe[i][j].w: S_efe_w[i][j][:], \n",
    "                                })\n",
    "                    \n",
    "                brian2.device.run(run_args=arguments, with_output=False) # process the sample\n",
    "                                \n",
    "                for ih in range(self.N_hidden):\n",
    "                    vt[ih] = self.exci_neurons[ih].vt[:]\n",
    "                        \n",
    "                if self.task == 'train' and self.switch_norm:\n",
    "                    S_input2e_w, S_efe_w = self.syn_weight_normalization()\n",
    "\n",
    "                switch_stimulus, stimulus_strength =  self.check_sample_status(img, label, stimulus_strength)\n",
    "                if self.debug==True:\n",
    "                    print(f'stimulus_idx: {stimulus_idx}, switch_stimulus: {switch_stimulus}, stimulus_strength: {stimulus_strength}, label: {label}')\n",
    "                    \n",
    "            # now the running of the stimulus meet the requirement, count the spikes for each digit\n",
    "            if stimulus_idx != 0: # the first events is fake\n",
    "                list_counts_for_digit_last_layer[stimulus_idx-1] = {'label': label, 'count':self.count_last_hlayer_spikes_from_spikemon()}\n",
    "\n",
    "        # save synapse, incl. connection, weight, pre and post index for training\n",
    "        if self.task == 'train':\n",
    "            self.save_synapses()\n",
    "            self.save_neurons()\n",
    "\n",
    "        if self.debug==True:\n",
    "            print(brian2.profiling_summary(self.col))\n",
    "            print(self.col.scheduling_summary())\n",
    "\n",
    "        eff_last_layer = 0  \n",
    "        eff_mult_match_last_layer = 0\n",
    "        for sidx, label_count in enumerate(list_counts_for_digit_last_layer):\n",
    "            max_value = max(label_count['count'])\n",
    "            label = label_count['label']\n",
    "            match = (label_count['count'][label] == max_value)\n",
    "            n_max = label_count['count'].count(max_value)\n",
    "            if match == True:\n",
    "                eff_last_layer +=1\n",
    "                if n_max > 1:\n",
    "                    eff_mult_match_last_layer +=1\n",
    "                \n",
    "            if self.root_out == True: # output ROOT files\n",
    "                for digit, counts in enumerate(label_count['count']):\n",
    "                    self.nt.Fill(sidx, label, digit, counts, float(match), n_max, self.rank)\n",
    "\n",
    "        eff_last_layer /= len(list_counts_for_digit_last_layer)\n",
    "        eff_mult_match_last_layer /= len(list_counts_for_digit_last_layer)\n",
    "\n",
    "        if self.root_out == True: \n",
    "            self.nt.Write()\n",
    "            self.root_file.Close()\n",
    "\n",
    "        return list_counts_for_digit_last_layer, eff_last_layer, eff_mult_match_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
