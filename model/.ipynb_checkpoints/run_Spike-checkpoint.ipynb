{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these command make the cell width wider than default\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Brian2.import_common_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams(n_hlayer):\n",
    "    try:\n",
    "        fixed_hyperpar_file = os.path.join(brian_dir, 'jobs_sub/ray_tune/fixed_hyperpar_file_base')\n",
    "        hyperparam = read_data(dir_name=fixed_hyperpar_file,\n",
    "                               worker_index=0,\n",
    "                               data_format='json',\n",
    "                               single_value=True)\n",
    "        print('--- model hyperpar loaded from ', fixed_hyperpar_file)\n",
    "    except:\n",
    "        sys.exit(f'!!!  {fixed_hyperpar_file} not found ---')\n",
    "    \n",
    "    return hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    debug = False\n",
    "    if debug == False:\n",
    "        logger = logging.getLogger(\"ray.data\")\n",
    "        logger.setLevel(logging.CRITICAL)\n",
    "        \n",
    "    activate_input_spikemon = False  # save input_spikes, only for debugging\n",
    "    idx_start_train = 0\n",
    "    idx_end_train = 10\n",
    "    idx_start_valid = 0 \n",
    "    idx_end_valid = 3\n",
    "    idx_start_test = 0\n",
    "    idx_end_test = 3\n",
    "    n_grp_train = 1 # e.g. n_grp_train = 2 for 0-1000, means run 0-500, 500-100 consecutively.\n",
    "    n_epoch = 1 \n",
    "    sqrt_grp_size = [1]  #sqrt(size) for each group in a layer\n",
    "    test_option = None\n",
    "    task_list = ['train', 'valid', 'test']\n",
    "    \n",
    "    n_hlayer = len(sqrt_grp_size)\n",
    "    hyperparams = get_hyperparams(n_hlayer)\n",
    "    \n",
    "    # MNIST data within given range\n",
    "    torchvision_data = fetch_torchvision_data(idx_start_train, idx_end_train,\n",
    "                       idx_start_valid, idx_end_valid,\n",
    "                       idx_start_test, idx_end_test, 'MNIST')\n",
    "    data_train, data_valid, data_test = torchvision_data.get_data_numpy()\n",
    "    \n",
    "    starttime = time.time()\n",
    "    nsample_train_per_grp = int((idx_end_train-idx_start_train)/n_grp_train) # n sample in each group\n",
    "    idx_start_prev = 0 # idx_start of previous group\n",
    "    idx_end_prev = 0 # idx_end of previous group\n",
    "    for epoch in range(n_epoch):\n",
    "        for igrp in range(n_grp_train):\n",
    "            #idx_start and idx_end of each group\n",
    "            idx_start = int(igrp*nsample_train_per_grp) \n",
    "            idx_end = idx_start + nsample_train_per_grp\n",
    "            previous_seg_name = 'seg_'+str(idx_start_prev)+'_'+str(idx_end_prev)\n",
    "            if idx_start >= len(data_train['label']): # run out of data\n",
    "                break\n",
    "            print(f'--------  epoch: {epoch}, group: {igrp} ------------')\n",
    "            data_train_subgrp = {key: value[idx_start:idx_end] for key, value in data_train.items()}\n",
    "            input_data = {'train':data_train_subgrp, 'valid': data_valid, 'test': data_test}\n",
    "            for task in task_list:\n",
    "                model = Spike_MNIST_Nlayer(\n",
    "                    task = task,\n",
    "                    idx_start_train = idx_start_train,\n",
    "                    idx_start = idx_start,\n",
    "                    idx_end = idx_end, \n",
    "                    simulation_duration = input_param.sim_time,\n",
    "                    epoch = epoch,\n",
    "                    previous_seg_name = previous_seg_name,\n",
    "                    sqrt_grp_size = sqrt_grp_size,\n",
    "                    test_option = test_option,\n",
    "                    hyperParams = hyperparams,\n",
    "                    debug = debug,\n",
    "                    activate_input_spikemon = False,\n",
    "                    root_out = True\n",
    "                )\n",
    "\n",
    "                _, eff_last_layer, eff_mult_match_last_layer = model.run(input_data[task])\n",
    "            \n",
    "                print(f'''\n",
    "                --- efficiency for {task} ---\n",
    "                eff_last_layer: {eff_last_layer}, eff_mult_match_last_layer: {eff_mult_match_last_layer}\n",
    "                ''')\n",
    "            # record indices from previous groups\n",
    "            idx_start_prev = idx_start + idx_start_train\n",
    "            idx_end_prev = idx_start_prev + nsample_train_per_grp\n",
    "            if idx_end_prev > idx_start_train+len(data_train['label']):\n",
    "                idx_end_prev = idx_start_train+len(data_train['label'])\n",
    "                \n",
    "    # clean all the unused files after all epoch runs\n",
    "    brian2.device.delete(force=True)\n",
    "    \n",
    "    endtime = time.time()\n",
    "    print('total time: ', endtime-starttime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
